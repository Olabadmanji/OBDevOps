<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Serverless Data Lake Engine | AWS Data Engineering</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 40px auto;
            max-width: 900px;
            line-height: 1.6;
            color: #333;
            background-color: white;
        }
        h1 {
            border-bottom: 3px solid #222;
            padding-bottom: 10px;
        }
        h2 {
            margin-top: 40px;
            color: #222;
        }
        .section {
            margin-top: 20px;
        }
        .highlight {
            background: #f5f5f5;
            padding: 15px;
            border-left: 4px solid #333;
            margin: 15px 0 20px 0;
        }
        ul {
            margin-left: 20px;
        }
        li {
            margin-bottom: 8px;
        }
        img {
            width: 100%;
            margin-top: 20px;
            border-radius: 4px;
        }
        .footer {
            margin-top: 60px;
            font-size: 14px;
            color: #777;
        }
        .footer a {
            text-decoration: none;
            color: #333;
            font-weight: 500;
        }
    </style>
</head>
<body>

    <h1>Serverless Data Lake Engine</h1>

    <p>
        A production-grade, end-to-end data engineering pipeline built on AWS. This system automates the ingestion of high-velocity streaming data, manages a multi-tier S3 Data Lake, and performs automated ETL to enable real-time business intelligence.
    </p>

    <h2>Project Objective</h2>
    <div class="section">
        <p>
            To design a scalable, serverless modern data architecture (MDA) that transforms raw, unstructured event data into optimized, queryable datasets while minimizing storage costs and operational overhead.
        </p>
    </div>

    <h2>Architecture Overview</h2>
    <div class="highlight">
        Kinesis Data Streams → Kinesis Firehose (Buffering) → S3 (Raw) → AWS Glue (PySpark ETL) → S3 (Processed) → Amazon Athena → QuickSight
    </div>

    

    <ul>
        <li><strong>Ingestion:</strong> Kinesis Data Streams capturing high-velocity JSON events.</li>
        <li><strong>Data Lake:</strong> S3 organized into 3 zones (Bronze/Silver/Gold) with time-based partitioning.</li>
        <li><strong>ETL:</strong> AWS Glue (PySpark) transforming raw JSON into optimized <strong>Apache Parquet</strong> format.</li>
        <li><strong>Analytics:</strong> Amazon Athena providing serverless SQL querying.</li>
    </ul>

    <h2>Key Technical Accomplishments</h2>
    <ul>
        <li><strong>80% Cost Reduction:</strong> Optimized Athena query performance and cost by implementing columnar Parquet storage and Snappy compression.</li>
        <li><strong>Infrastructure as Code:</strong> Fully automated the deployment of Kinesis, S3, Glue, and IAM roles using AWS CloudFormation.</li>
        <li><strong>Schema-on-Read:</strong> Configured AWS Glue Crawlers to automatically handle schema evolution and maintain the Data Catalog.</li>
        <li><strong>Data Integrity:</strong> Implemented specialized IAM policies to ensure secure, least-privilege access across the serverless stack.</li>
    </ul>

    <h2>Project Repository</h2>
    <p>
        <a href="https://github.com/Olabadmanji/cloud-devops/tree/main/Automated%20ETL%20Pipeline" target="_blank">
            View Full Documentation & Infrastructure Code on GitHub →
        </a>
    </p>

    <div class="footer">
        Built by Olabanji | Cloud & DevOps Engineer <br><br>
        <a href="https://olabadmanji.github.io/cloud-devops/">← Back to portfolio</a>
    </div>

</body>
</html>